{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b19328",
   "metadata": {},
   "source": [
    "## 🎯 Introduction: Preparing the Titanic Dataset for Linear Regression\n",
    "\n",
    "In this notebook, we will walk through the **end-to-end preprocessing and modeling pipeline** to train a **Linear Regression** model on the Titanic dataset. Our goal is to prepare the data in a clean, consistent, and ML-friendly format that works well for a linear model.\n",
    "\n",
    "> 🧠 Note: Steps are not always followed in strict order — some may overlap, repeat, or depend on the dataset's unique structure.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Key Steps in the Pipeline:\n",
    "\n",
    "- **Load the dataset**  \n",
    "  Import the Titanic dataset using `pandas`.\n",
    "\n",
    "- **Inspect the dataset**  \n",
    "  Check shape, column types, null values, and preview initial rows.\n",
    "\n",
    "- **Handle missing values**  \n",
    "  Use techniques like filling (mean/median/mode) or flagging missing entries.\n",
    "\n",
    "- **Remove duplicates**  \n",
    "  Ensure data integrity by dropping exact duplicate rows if any.\n",
    "\n",
    "- **Fix incorrect data types**  \n",
    "  Convert columns to proper formats (e.g., object → numeric, date, category).\n",
    "\n",
    "- **Encode categorical variables**  \n",
    "  Use label encoding or one-hot encoding depending on the variable type and model requirement.\n",
    "\n",
    "- **Feature engineering**  \n",
    "  Derive new useful features such as extracting deck from cabin, family size, etc.\n",
    "\n",
    "- **Handle outliers** *(optional)*  \n",
    "  Detect and optionally remove or treat extreme values in numerical features.\n",
    "\n",
    "- **Normalize / Scale numerical features**  \n",
    "  Especially important for Linear Regression to stabilize feature impact.\n",
    "\n",
    "- **Split the dataset**  \n",
    "  Divide into training and testing sets (optionally validation too).\n",
    "\n",
    "- **Balance the dataset** *(optional for classification)*  \n",
    "  Apply SMOTE or class weights if doing classification (e.g., survival prediction).\n",
    "\n",
    "- **Train the model**  \n",
    "  Fit a linear regression model on the cleaned training data.\n",
    "\n",
    "- **Tune hyperparameters (if any)**  \n",
    "  Although linear regression has few, regularization (Ridge/Lasso) may be explored.\n",
    "\n",
    "- **Evaluate performance**  \n",
    "  Measure accuracy, RMSE, or R² depending on the task type.\n",
    "\n",
    "---\n",
    "\n",
    "> 💡 Titanic is typically used for classification (survived vs not), but this workflow can also demonstrate regression-style preprocessing or be extended to logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b660b65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "470c2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# reason why we dont import scikit-learn bc \n",
    "#1. its a package\n",
    "#2. (-) hiphens are not allowed in python  in library names\n",
    "# so instead we import the specific function we need , can aslo do import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b639bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/aman/Desktop/datascience/dataset/Titanic-Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508f2e1",
   "metadata": {},
   "source": [
    "# inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "801f5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "# inspecting the data\n",
    "# what data is ?\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5d597787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aee56cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unwanted data like ,sibsp , ticket , fare ,name,pasanger id \n",
    "\n",
    "# df.drop([\"PassengerId\",\"Name\",\"SibSp\",\"Ticket\",\"Fare\"],axis=1)\n",
    "\n",
    "\n",
    "#note -> if u run the code above more than once (in same working instance) , this throws error  KeyError: \"['PassengerId', 'Name', 'SibSp', 'Ticket', 'Fare'] not found in axis\" : bc its already removed that rows so running code again throws the error , so in order to remove that Keyerror case \n",
    "df =df.drop([\"PassengerId\",\"Name\",\"SibSp\",\"Ticket\",\"Fare\"],axis=1,errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9c77a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 7)\n",
      "----------------------------------------------\n",
      "Survived      0\n",
      "Pclass        0\n",
      "Sex           0\n",
      "Age         177\n",
      "Parch         0\n",
      "Cabin       687\n",
      "Embarked      2\n",
      "dtype: int64\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(\"----------------------------------------------\")\n",
    "print(df.isnull().sum()) # checking for null values in each column\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d378f52",
   "metadata": {},
   "source": [
    "# setp-2 handling missing values\n",
    "1. removing data itself (easy)\n",
    "2. imputation (filling missing values) -> \n",
    "`constant`\n",
    "`mean,meadian,mode`\n",
    "`forwardfill,backwaedfill`\n",
    "3. model based filling -> `KNN` `REGRESSION`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0bd4e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "Parch       0\n",
      "Cabin       0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "(183, 7)\n"
     ]
    }
   ],
   "source": [
    "# REMOVING MISSING VALUES\n",
    "df1 = df.dropna() # removing rows with missing values\n",
    "print(df1.isnull().sum()) # checking for null values in each column after removing rows\n",
    "print(df1.shape)\n",
    "# so much data is lost , here in this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "942da411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "Parch       0\n",
      "Cabin       0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "(891, 7)\n",
      "----------------------------------------------\n",
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "Parch       0\n",
      "Cabin       0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "(891, 7)\n",
      "----------------------------------------------\n",
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "Parch       0\n",
      "Cabin       1\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "(891, 7)\n",
      "----------------------------------------------\n",
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "Parch       0\n",
      "Cabin       1\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "(891, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60496/2120702200.py:14: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df4=df.fillna(method='ffill') # forward fill\n",
      "/tmp/ipykernel_60496/2120702200.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df5=df.fillna(method='bfill') # backward fill\n"
     ]
    }
   ],
   "source": [
    "# imputation\n",
    "# filling missing values with mean\n",
    "df2 = df.fillna(df['Age'].mean()) # filling missing values with mean\n",
    "print(df2.isnull().sum()) # checking for null values in each column after filling missing values\n",
    "print(df2.shape) #DATA IS MAINTAINED\n",
    "df3=df.fillna(df['Age'].median())\n",
    "print('----------------------------------------------')\n",
    "print(df3.isnull().sum()) # checking for null values in each column after filling missing values\n",
    "print(df3.shape) #DATA IS MAINTAINED\n",
    "\n",
    "# so for mode\n",
    "\n",
    "# forward fill and backward fill\n",
    "df4=df.fillna(method='ffill') # forward fill\n",
    "print('----------------------------------------------')\n",
    "print(df4.isnull().sum()) # checking for null values in each column after filling missing values\n",
    "print(df4.shape) #DATA IS MAINTAINED\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df5=df.fillna(method='bfill') # backward fill\n",
    "print('----------------------------------------------')\n",
    "print(df5.isnull().sum()) # checking for null values in each column after filling missing values\n",
    "print(df5.shape) #DATA IS MAINTAINED\n",
    "\n",
    "\n",
    "#note cabin have 1 missing value , still left bc its the first row "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5957429d",
   "metadata": {},
   "source": [
    "## knn imputer \n",
    "\n",
    "he KNN Imputer is a method used to fill missing values in a dataset using the K-Nearest Neighbors approach. It's available in scikit-learn (sklearn.impute.KNNImputer). Instead of using simple strategies like mean or median imputation, KNN Imputer looks at the nearest data points (neighbors) and fills missing values based on their values.\n",
    "🔧 How It Works\n",
    "\n",
    "For each missing value in a feature:\n",
    "\n",
    "    It finds the k-nearest samples (rows) that have a value for that feature.\n",
    "\n",
    "    It uses the average (or weighted average) of those neighbors to impute the missing value.\n",
    "        KNNImputer(...) creates an instance of the KNNImputer class from sklearn.impute.\n",
    "\n",
    "    imputer is just a variable name that holds this instance.\n",
    "\n",
    "    This instance has methods like .fit(), .transform(), and .fit_transform() that are used to impute missing values.\n",
    "\n",
    "So technically:\n",
    "\n",
    "    🔹 imputer is an object of the KNNImputer class that you use to perform K-Nearest Neighbors-based imputation on a dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### .fit()\n",
    "\n",
    "\n",
    "\n",
    "imputer.fit(data)\n",
    "\n",
    "    Learns from the data.\n",
    "\n",
    "    In KNNImputer, this means it calculates distances between rows, finds neighbors, and prepares itself to do imputation.\n",
    "\n",
    "    But it does NOT actually change or return the data yet.\n",
    "\n",
    "💬 Think of it as:\n",
    "\n",
    "    “Hey imputer, look at this data and understand how it’s structured.”\n",
    "\n",
    "###  .transform()\n",
    "\n",
    "imputed_data = imputer.transform(data)\n",
    "\n",
    "    Uses what was learned in .fit() to actually fill in the missing values.\n",
    "\n",
    "    It gives you back a version of the dataset where NaNs are replaced.\n",
    "\n",
    "💬 Think of it as:\n",
    "\n",
    "    “Okay, now apply what you learned and fix the missing values.”\n",
    "###  .fit_transform()\n",
    "\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "    Does both steps in one line:\n",
    "\n",
    "        First .fit(data) to learn\n",
    "\n",
    "        Then .transform(data) to return the imputed data\n",
    "## important point knnimputer returns numpy array not data frame so u have to convert it into data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "563dff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model based filling\n",
    "#KNN imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "#so from sklearn lib , importing knnimputer class , creating a instance from that class knn imputer with n_neighors , so imputer is object , when initiated will find the missing value row find 4 similar rows of that then average it and put value , in aloop it will do that for all \n",
    "imputer = KNNImputer(n_neighbors=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983e3ee",
   "metadata": {},
   "source": [
    "###  note\n",
    " knn imputer can only be used on numerical values so either convert the df into numeric_df or drop those columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddef877",
   "metadata": {},
   "source": [
    "✅ 2. What does \"partial encoding\" mean in this case?\n",
    "\n",
    "“Partial encoding” means:\n",
    "\n",
    "    We encode only the columns that help with similarity and are relevant for modeling or imputation.\n",
    "\n",
    "This is good practice because:\n",
    "\n",
    "    You keep useful categorical info (Sex, Embarked)\n",
    "\n",
    "    You avoid noise from irrelevant fields (Name, Ticket)\n",
    "\n",
    "🚫 Why encoding everything can hurt KNNImputer:\n",
    "\n",
    "KNNImputer fills missing values by comparing rows using distances. If you include random or non-informative columns, KNN will calculate wrong neighbors — leading to bad imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ebfb7f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass     Sex   Age  Parch Cabin Embarked\n",
      "0         0       3    male  22.0      0   NaN        S\n",
      "1         1       1  female  38.0      0   C85        C\n",
      "2         1       3  female  26.0      0   NaN        S\n",
      "3         1       1  female  35.0      0  C123        S\n",
      "4         0       3    male  35.0      0   NaN        S\n",
      "---------------after encoding----------\n",
      "   Survived  Pclass   Age  Parch Cabin  Sex_male  Embarked_Q  Embarked_S\n",
      "0         0       3  22.0      0   NaN      True       False        True\n",
      "1         1       1  38.0      0   C85     False       False       False\n",
      "2         1       3  26.0      0   NaN     False       False        True\n",
      "3         1       1  35.0      0  C123     False       False        True\n",
      "4         0       3  35.0      0   NaN      True       False        True\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "# encoding \n",
    "df = pd.get_dummies(df,columns=['Sex','Embarked'],drop_first=True)\n",
    "print(f'---------------after encoding----------')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bda6f1",
   "metadata": {},
   "source": [
    "#### note for code above \n",
    "📝 drop_first in One-Hot Encoding — Notes\n",
    "🔹 What is drop_first?\n",
    "\n",
    "    drop_first=True is a parameter used in pd.get_dummies() to drop the first category from each categorical column during one-hot encoding.\n",
    "\n",
    "🔹 Why use drop_first=True?\n",
    "\n",
    "    To avoid multicollinearity — especially important in linear models like Logistic or Linear Regression.\n",
    "\n",
    "    Because when you have n categories, one of them can be inferred from the rest.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    Colors: Red, Blue, Green\n",
    "\n",
    "    One-hot encoding (drop_first=False):\n",
    "      Red | Blue | Green\n",
    "\n",
    "    One-hot encoding (drop_first=True):\n",
    "      Blue | Green\n",
    "      (If both are 0 → it's Red)\n",
    "\n",
    "    This makes the dataset simpler, avoids redundant information, and improves stability for some models.\n",
    "\n",
    "🔹 When to use it?\n",
    "Use drop_first=True if:\tUse drop_first=False if:\n",
    "✅ Using linear models\t✅ Using tree-based or KNN models\n",
    "✅ Want to avoid multicollinearity\t✅ Want full feature flexibility\n",
    "✅ Dataset is small/simple\t✅ Dataset is large and tree-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f629b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2516afea",
   "metadata": {},
   "source": [
    "## 🧠 Titanic Dataset — How to Handle the `Cabin` Column prbolem\n",
    "\n",
    "### 🔍 The Problem:\n",
    "- `Cabin` contains values like \"C85\", \"B57\", \"E46\", etc.\n",
    "- Most values are **unique** (high-cardinality) and **many are missing**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why NOT to Use Full `Cabin` Values:\n",
    "- ⚠️ Most `Cabin` values appear **only once** — no pattern to learn from.\n",
    "- ⚠️ Encoding these will create **hundreds of dummy columns** (one per unique cabin).\n",
    "- ⚠️ Models may **overfit** on meaningless, rare values.\n",
    "- ⚠️ Increases memory and computation without real gain.\n",
    "\n",
    "➡️ Example: \"C85\" and \"C86\" may look similar, but there's no proof they mean anything close.  \n",
    "➡️ One-hot encoding full `Cabin` values = **waste of features**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why NOT to Impute Using KNN:\n",
    "- `Cabin` (or even extracted deck) is **categorical**.\n",
    "- KNN works on **numeric similarity** — it can't measure \"how close\" decks B and C are.\n",
    "- Imputing deck like a number may assign wrong values, misleading the model.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ What TO DO Instead:\n",
    "- ✅ Extract the **deck letter** from `Cabin` (e.g., `'C'` from `\"C85\"`).\n",
    "- ✅ Replace missing values with `\"U\"` (unknown deck).\n",
    "- ✅ Apply **One-Hot Encoding** on the deck letters.\n",
    "\n",
    "This gives a small number of useful features (e.g., Deck_A, Deck_B, ..., Deck_U) instead of hundreds of garbage columns.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Final Note:\n",
    "> Treat `Cabin` like a noisy text column.  \n",
    "> Only keep it if simplified into **deck letters**, handled carefully.  \n",
    "> Never encode full values or impute with KNN — this can hurt model accuracy and generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "debd2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------encode-------------------------\n",
      "   Survived  Pclass   Age  Parch  Sex_male  Embarked_Q  Embarked_S  Deck_B  \\\n",
      "0         0       3  22.0      0      True       False        True   False   \n",
      "1         1       1  38.0      0     False       False       False   False   \n",
      "2         1       3  26.0      0     False       False        True   False   \n",
      "3         1       1  35.0      0     False       False        True   False   \n",
      "4         0       3  35.0      0      True       False        True   False   \n",
      "\n",
      "   Deck_C  Deck_D  Deck_E  Deck_F  Deck_G  Deck_T  Deck_U  \n",
      "0   False   False   False   False   False   False    True  \n",
      "1    True   False   False   False   False   False   False  \n",
      "2   False   False   False   False   False   False    True  \n",
      "3    True   False   False   False   False   False   False  \n",
      "4   False   False   False   False   False   False    True  \n"
     ]
    }
   ],
   "source": [
    "#solving cabin issue \n",
    "\n",
    "df['Deck'] = df['Cabin'].str[0]  \n",
    "df['Deck'] = df['Deck'].fillna('U') \n",
    "\n",
    "#drop 'Cabin' column (errors='ignore' prevents crash if 'Cabin' doesn't exist)\n",
    "df = df.drop(columns='Cabin', errors='ignore')  \n",
    "\n",
    "# if you run this entire cell TWICE in the same session:\n",
    "# ⚠️ It will crash on the first line:\n",
    "#df['Deck'] = df['Cabin'].str[0]\n",
    "#     ⛔ Because 'Cabin' was already dropped in the first run\n",
    "#     So now df['Cabin'] does not exist → KeyError\n",
    "# ❌ Note: errors='ignore' ONLY applies to drop(), not to df['Cabin']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# also encoding DECK NOW \n",
    "df = pd.get_dummies(df, columns=['Deck'], drop_first=True)\n",
    "\n",
    "\n",
    "print(\"---------------encode-------------------------\")\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3f4ed620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived      0\n",
      "Pclass        0\n",
      "Age           0\n",
      "Parch         0\n",
      "Sex_male      0\n",
      "Embarked_Q    0\n",
      "Embarked_S    0\n",
      "Deck_B        0\n",
      "Deck_C        0\n",
      "Deck_D        0\n",
      "Deck_E        0\n",
      "Deck_F        0\n",
      "Deck_G        0\n",
      "Deck_T        0\n",
      "Deck_U        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# bc imputer return numpy array so we also have to convert it in notes above\n",
    "numpydf = imputer.fit_transform(df)\n",
    "df = pd.DataFrame(numpydf, columns=df.columns)\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1296aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
