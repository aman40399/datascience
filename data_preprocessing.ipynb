{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b19328",
   "metadata": {},
   "source": [
    "\n",
    "# data_preprocess \n",
    " ## steps \n",
    "    Load the dataset\n",
    "\n",
    "    Inspect the dataset (shape, types, nulls, preview)\n",
    "\n",
    "    Handle missing values\n",
    "\n",
    "    Remove duplicates\n",
    "\n",
    "    Fix incorrect data types\n",
    "\n",
    "    Encode categorical variables\n",
    "\n",
    "    Feature engineering (create/update features)\n",
    "\n",
    "    Handle outliers (optional)\n",
    "\n",
    "    Normalize / Scale numerical features\n",
    "\n",
    "    Split dataset into train/test (and optionally validation)\n",
    "\n",
    "    Balance dataset (SMOTE or class weights) [optional for classification]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b660b65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470c2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# reason why we dont import scikit-learn bc \n",
    "#1. its a package\n",
    "#2. (-) hiphens are not allowed in python  in library names\n",
    "# so instead we import the specific function we need , can aslo do import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b639bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/aman/Desktop/datascience/dataset/Titanic-Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508f2e1",
   "metadata": {},
   "source": [
    "# inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801f5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "# inspecting the data\n",
    "# what data is ?\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d597787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aee56cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex   Age  Parch Cabin Embarked\n",
       "0           0       3    male  22.0      0   NaN        S\n",
       "1           1       1  female  38.0      0   C85        C\n",
       "2           1       3  female  26.0      0   NaN        S\n",
       "3           1       1  female  35.0      0  C123        S\n",
       "4           0       3    male  35.0      0   NaN        S\n",
       "..        ...     ...     ...   ...    ...   ...      ...\n",
       "886         0       2    male  27.0      0   NaN        S\n",
       "887         1       1  female  19.0      0   B42        S\n",
       "888         0       3  female   NaN      2   NaN        S\n",
       "889         1       1    male  26.0      0  C148        C\n",
       "890         0       3    male  32.0      0   NaN        Q\n",
       "\n",
       "[891 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing unwanted data like ,sibsp , ticket , fare ,name,pasanger id \n",
    "\n",
    "# df.drop([\"PassengerId\",\"Name\",\"SibSp\",\"Ticket\",\"Fare\"],axis=1)\n",
    "\n",
    "\n",
    "#note -> if u run the code above more than once (in same working instance) , this throws error  KeyError: \"['PassengerId', 'Name', 'SibSp', 'Ticket', 'Fare'] not found in axis\" : bc its already removed that rows so running code again throws the error , so in order to remove that Keyerror case \n",
    "df.drop([\"PassengerId\",\"Name\",\"SibSp\",\"Ticket\",\"Fare\"],axis=1,errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c77a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "----------------------------------------------\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(\"----------------------------------------------\")\n",
    "print(df.isnull().sum()) # checking for null values in each column\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d378f52",
   "metadata": {},
   "source": [
    "# setp-2 handling missing values\n",
    "1. removing data itself (easy)\n",
    "2. imputation (filling missing values) -> \n",
    "`constant`\n",
    "`mean,meadian,mode`\n",
    "`forwardfill,backwaedfill`\n",
    "3. model based filling -> `KNN` `REGRESSION`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bd4e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "(183, 12)\n"
     ]
    }
   ],
   "source": [
    "# REMOVING MISSING VALUES\n",
    "df1 = df.dropna() # removing rows with missing values\n",
    "print(df1.isnull().sum()) # checking for null values in each column after removing rows\n",
    "print(df1.shape)\n",
    "# so much data is lost , here in this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942da411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "(891, 12)\n",
      "----------------------------------------------\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "(891, 12)\n",
      "----------------------------------------------\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          1\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "(891, 12)\n",
      "----------------------------------------------\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          1\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "(891, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7074/2120702200.py:14: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df4=df.fillna(method='ffill') # forward fill\n",
      "/tmp/ipykernel_7074/2120702200.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df5=df.fillna(method='bfill') # backward fill\n"
     ]
    }
   ],
   "source": [
    "# imputation\n",
    "# filling missing values with mean\n",
    "df2 = df.fillna(df['Age'].mean()) # filling missing values with mean\n",
    "print(df2.isnull().sum()) # checking for null values in each column after filling missing values\n",
    "print(df2.shape) #DATA IS MAINTAINED\n",
    "df3=df.fillna(df['Age'].median())\n",
    "print('----------------------------------------------')\n",
    "print(df3.isnull().sum()) # checking for null values in each column after filling missing values\n",
    "print(df3.shape) #DATA IS MAINTAINED\n",
    "\n",
    "# so for mode\n",
    "\n",
    "# forward fill and backward fill\n",
    "df4=df.fillna(method='ffill') # forward fill\n",
    "print('----------------------------------------------')\n",
    "print(df4.isnull().sum()) # checking for null values in each column after filling missing values\n",
    "print(df4.shape) #DATA IS MAINTAINED\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df5=df.fillna(method='bfill') # backward fill\n",
    "print('----------------------------------------------')\n",
    "print(df5.isnull().sum()) # checking for null values in each column after filling missing values\n",
    "print(df5.shape) #DATA IS MAINTAINED\n",
    "\n",
    "\n",
    "#note cabin have 1 missing value , still left bc its the first row "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5957429d",
   "metadata": {},
   "source": [
    "## knn imputer \n",
    "\n",
    "he KNN Imputer is a method used to fill missing values in a dataset using the K-Nearest Neighbors approach. It's available in scikit-learn (sklearn.impute.KNNImputer). Instead of using simple strategies like mean or median imputation, KNN Imputer looks at the nearest data points (neighbors) and fills missing values based on their values.\n",
    "üîß How It Works\n",
    "\n",
    "For each missing value in a feature:\n",
    "\n",
    "    It finds the k-nearest samples (rows) that have a value for that feature.\n",
    "\n",
    "    It uses the average (or weighted average) of those neighbors to impute the missing value.\n",
    "        KNNImputer(...) creates an instance of the KNNImputer class from sklearn.impute.\n",
    "\n",
    "    imputer is just a variable name that holds this instance.\n",
    "\n",
    "    This instance has methods like .fit(), .transform(), and .fit_transform() that are used to impute missing values.\n",
    "\n",
    "So technically:\n",
    "\n",
    "    üîπ imputer is an object of the KNNImputer class that you use to perform K-Nearest Neighbors-based imputation on a dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### .fit()\n",
    "\n",
    "\n",
    "\n",
    "imputer.fit(data)\n",
    "\n",
    "    Learns from the data.\n",
    "\n",
    "    In KNNImputer, this means it calculates distances between rows, finds neighbors, and prepares itself to do imputation.\n",
    "\n",
    "    But it does NOT actually change or return the data yet.\n",
    "\n",
    "üí¨ Think of it as:\n",
    "\n",
    "    ‚ÄúHey imputer, look at this data and understand how it‚Äôs structured.‚Äù\n",
    "\n",
    "###  .transform()\n",
    "\n",
    "imputed_data = imputer.transform(data)\n",
    "\n",
    "    Uses what was learned in .fit() to actually fill in the missing values.\n",
    "\n",
    "    It gives you back a version of the dataset where NaNs are replaced.\n",
    "\n",
    "üí¨ Think of it as:\n",
    "\n",
    "    ‚ÄúOkay, now apply what you learned and fix the missing values.‚Äù\n",
    "###  .fit_transform()\n",
    "\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "    Does both steps in one line:\n",
    "\n",
    "        First .fit(data) to learn\n",
    "\n",
    "        Then .transform(data) to return the imputed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563dff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model based filling\n",
    "#KNN imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "#so from sklearn lib , importing knnimputer class , creating a instance from that class knn imputer with n_neighors , so imputer is object , when initiated will find the missing value row find 4 similar rows of that then average it and put value , in aloop it will do that for all \n",
    "imputer = KNNImputer(n_neighbors=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983e3ee",
   "metadata": {},
   "source": [
    "###  note\n",
    " knn imputer can only be used on numerical values so either convert the df into numeric_df or drop those columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddef877",
   "metadata": {},
   "source": [
    "‚úÖ 2. What does \"partial encoding\" mean in this case?\n",
    "\n",
    "‚ÄúPartial encoding‚Äù means:\n",
    "\n",
    "    We encode only the columns that help with similarity and are relevant for modeling or imputation.\n",
    "\n",
    "This is good practice because:\n",
    "\n",
    "    You keep useful categorical info (Sex, Embarked)\n",
    "\n",
    "    You avoid noise from irrelevant fields (Name, Ticket)\n",
    "\n",
    "üö´ Why encoding everything can hurt KNNImputer:\n",
    "\n",
    "KNNImputer fills missing values by comparing rows using distances. If you include random or non-informative columns, KNN will calculate wrong neighbors ‚Äî leading to bad imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb7f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ed620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "print(df.isnull.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
